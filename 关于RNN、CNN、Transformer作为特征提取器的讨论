RNN-廉颇老矣，尚能饭否
  RNN成为主流NLP特征提取器的主要原因如下：
  1. RNN本身结构就可以接纳不定长输入的由前向后进行信息线性传导的网络结构
  2. 很好的保留了语言(句子)的结构特性

RNN在新时代面临的两个问题
  1. 后起之秀的模型，比如经过特殊改造的CNN模型以及最近特别流行的Transformer
  2. RNN的并行能力太差

RNN在并行能力上的改进方法
  1. Simple Recurrent Units for Highly Parallelizable Recurrence它最本质的改进是把隐层之间的神经元依赖由全连接改成了哈达马乘积，
     这样 T 时刻隐层单元本来对 T-1 时刻所有隐层单元的依赖，改成了只是对 T-1 时刻对应单元的依赖。
  2. 为了能够在不同时间步输入之间进行并行计算，那么只有一种做法，那就是打断隐层之间的连接，但是又不能全打断，因为这样基本就无法捕获组合特征了，
     所以唯一能选的策略就是部分打断， 比如每隔 2 个时间步打断一次，但是距离稍微远点的特征如何捕获呢？只能加深层深，通过层深来建立远距离特征之间的联系。
     参考Sliced Recurrent Neural Network，据说是CNN的简化版？


CNN-偏师之将
  NLP中早期的怀旧版CNN
  最早将CNN引入NLP的是Kim在2014年做的工作。
